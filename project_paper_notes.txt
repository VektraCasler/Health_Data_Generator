-------------------------------------
Questions to answer about the project
-------------------------------------

1.  What is this project?

This is a procedural patient clinical testing data generator.  It uses pseudo-randomized number generation to generate patient data fields, based on weighted choices (where possible) and mathematical formulae.  Weighted choices incorporate data from internet sources to approximate population distributions which resemble real-world data as closely as possible.  Mathematical formulae model probability distribution functions, such as normal curves, inverse relationships, or surge functions where possible.  These formulae are then used to generate patient data, which, when taken in aggregate, would demonstrate probability distributions of those types.  

Ultimately, this script simulates a population of patients which undergo clinical testing over various analytes.  Patients are either admitted to a theoretical hospital for a stay of procedurally determined length, or remain outpatient and are chosen at random to undergo procedural clinical testing.  The output of this script is a large collection of medical data -- a flat database (comma-separated value file), which holds dozens of fields for millions of records.

2.  Why did you create it?

Patient health information enjoys protections which minimize risk to the patients.  Unscrupulous use of this data can be damaging and detrimental to the health and well-being of the individual, and so it is protected by several laws and governing bodies, not least of which is the Health Insurance Portability and Accountability Act (HIPAA) under the watchful eye of the US Department of Health and Human Services.  Health care providers are beholden to these laws and have restricted access to patient data for testing or research purposes.  This can create a paradoxical impediment to data access: investigators can't prepare for the data of what they're attempting to investigate without having previously encountered the data.  This tool can help remove this barrier to entry at the initiation of the investigation, by allowing them to see what manner of data fields are typically available in patient clinical testing records.

3.Who is it for?

This data generation tool is for anyone interested in data science investigations in healthcare.  Data investigators can use it to build analysis pipelines, in preparation for using live data.  Residents could use it to generate QA project investigations.  Anyone who currently has or would like to develop data visualization or processing skills will be able to use this tool to generate a very large amount of data.

4.  How will it help?

Synthetic data absolutely removes any risk of patient identification.  Even 'deidentified' data can still be traced back to the original patient in some instances.  (Example: Even if they contain no patient identifiers, certain specimen photos could be linked to their patients.)  Synthetic clinical data has no associate patient.  They are all procedurally generated.

5.  What can people do with this synthetic data?

Example projects include:
   - Testing usage studies for assessing 'break-even' points in cost analysis.
   - Investigate timestamps to assess bottlenecks or behaviors.
   - Ordering quality studies for assessing clinician performance and ordering habits. 
   - Investigate correlations between patient diagnoses and clinical values.
   - Construct clinical investigatory pipelines in preparation for actual data.
   - Bolster AI training pools with "true negative" data, minimizing the need for pulling real patient data.
   - Facilitate AI quality control or validation studies.
   - Build dashboards for any of these investigations.
   - Organize learning modules for trainees around assessing data and various quality metrics (L-J plots, Youden charts, etc.)

5b.  Who else is curently doing this?

I've encountered one commercial enterprise that provides synthetic healthcare data for testing.  I'm currently awaiting an appointment to talk with them about their data generation capabilities and focus of their product.  From the cursory website information, this appears to be more of an EMR type of data generation, for generating patient encounters and notes, as opposed to clinical laboratory data, which this project focuses on.  They have published a few papers on their validation methods of their data, as well as how their AI tools for generating the data were themselves created.

6.  How did you create this project?

This project uses the Python programming language, as well as a few popular modules such as Pandas (for flat-data structure manipulation), 'random' (for pseudo-random number generation), and math (for designing mathematical probability distribution functions).

Since the product of this tool is a flat data structure (comma-separated value file), I used knowledge of prior projects of this type to recall datafields commonly encountered in clinical lab testing databases to decide which columns should be generated.

7.  Which data sources did you use?

    DEMOGRAPHIC DATA

        https://www.census.gov/prod/cen2010/briefs/c2010br-03.pdf
        https://www.census.gov/quickfacts/fact/table/US/PST045219
        https://www.census.gov/quickfacts/fact/note/US/RHI625219
        https://geographic.org/streetview/usa/index.html
        https://www.healthit.gov/isa/uscdi-data-class/patient-demographics#uscdi-v1
        https://blog.splitwise.com/2013/09/18/the-2010-us-census-population-by-zip-code-totally-free/
        https://www.ssa.gov/oact/babynames/names.zip
        https://namecensus.com/most_common_surnames.htm
        
    TEST ORDERING DATA

        https://www.cms.gov/Medicare/Coding/ICD10/2020-ICD-10-CM
        https://www.aamc.org/data-reports/workforce/interactive-data/number-people-active-physician-specialty-2017
        https://www.ahrq.gov/research/findings/factsheets/primary/pcwork3/index.html
        https://loinc.org/downloads/loinc-table/

    LAB VALUE DATA

        URMC internal documents
        Tietz Manual of Chemistry

    MISCELLANEOUS DATA

        https://email-verify.my-addr.com/list-of-most-popular-email-domains.php
        https://www.pewresearch.org/internet/fact-sheet/mobile/#:~:text=Mobile%20phone%20ownership%20over%20time,a%20cellphone%20of%20some%20kind.
        https://www.statista.com/chart/2072/landline-phones-in-the-united-states/
        https://en.wikipedia.org/wiki/List_of_North_American_Numbering_Plan_area_codes
        https://en.wikipedia.org/wiki/Telephone_prefix#United_States_prefixes

8.  What data points does it generate?

<Insert master_column_list.txt>

9.  What are some features of this generation?

As much as is possible, I'm using real-world data to simulate this generated data in a procedural fashion, rather than a simply random one.  Certain values have weights which prompt them to arise more commonly than others in the list of choices.

Likewise, I'm modeling the test results to match healthy population distributions as closely as possible.  Typically, this will mean a normal curve distribution across a range of values from low to high (chemistries like K+ or Na+, for example), or other mathematical curves, such as an inverse relationship (troponins).  Certain analytes may have prominent skew or kurtosis to their normal curve distributions.

10.  How does simulated data compare to real data?

So far, it matches the original data sets fairly well, allowing for statistical variation.  However, I hope to expand this testing to verification against actual clinical data.  When this is performed, I expect some discrepancies will arise (detailed below).

Eventually, I'd like to perform a machine learning analysis on this generated data and actual data, to see if the algorythm can distinguish the sythetic from the real.  And if it can, to what confidence level.  (Getting a machine to NOT find something accurately is a paradox in itself to contend with.)

11.  What are some limitations of this simulated data?

First, only healthy population distributions are generated.  Patient testing results are specifically designed to be normally distributed across the normal range.  This is intentional, to generate negative data for machine learning purposes.

No disease process bias is inherent in this procedural generation.  There exist no factual medical biases, such as links between patient demographics and medical testing ranges.  Again, by design, but also necessity, as maintaining and instituting these biases would require effort equal to discovering them in real data in the first place.

Much of the weighted data relies currently on found internet sources.  Though these sources may be reputable (The US Office of Social Security) or semi-reputable (other investigations performed by data scientists), it still relies upon published sources.  

Specific project scope limitations were also erected. (Only USA addresses, imperial units for value reporting, certain demographics were limited due to diminishing usefulness.)

12.  How will you distribute this tool to other users?

For now, Github.  This is an online project version tracking website.  It serves as a repository of project files and data.  Project members can pull, push, or fork project files as they wish.  This form of distribution has little ability to track usage of the project, something I'm interested in doing in order to see how the tool is used.  People can just silently clone the project and leave, with no feedback as to how they're using it.

13.  How will you support this project long term?

In the long term, I'd love to build this into some sort of hosted javascript web application, perhaps on an official URMC site.  This would offer more robust usage tracking.  I don't yet know how the project might be supported if I were to depart URMC, however.  I'd be interested in suggestions on how to generate longitudinal support from perhaps residents or data science graduate students.  Collaboration with UR or RIT would be beneficial in this.

Changing to a different data structure or programming language may be necessary.  I'm currently investigating how a relational database structure, such as SQL, might benefit this project.

14.  What features do you wish to add to this project?

More Data Types: Future iterations of this project will also include expansions of datafields generated.  Most of my experience is with clinical laboratory field data at the moment, but with 90K LOINC codes, any manner of health care testing data (which fits into a flat file format) could be generated.  Future data outputs might include synthetic molecular data for constructing NGS pipelines, or graphical data, such as heart rhythm tracings or other imaging data.

Optional flags to generate different types of data for software/script error testing.  Perhaps inject 'corrupted' data fields with missing/null values, non-standard characters in text fields, and 'data dirtying' subroutines to allow pipeline developers to construct data-wrangling scripts and tools.

Data field toggles: One important feature would be the ability to turn on and off various datafields which are unlikely to be necessary in whatever investigation the user is undertaking.  Flat data files compound their file sizes rapidly for each data field added.  File sizes could quickly become unusably large in this instance.

15.  How does this project promote the scientific community?  

I'm not yet entirely sure.  I'm currently wrestling with "how to I make this into research?"  The simplest question seems to be the most important: What is this project good for?  And to answer that, I'd need to research into how this project gets used.  I'm currently brainstorming for ideas on how best to investigate or track that.  

16.  Where would you publish this research?